#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
{{ agent_name }}

A web scraping agent generated by AutoGenesis.
"""

import os
import sys
import logging
import requests
from datetime import datetime
import pandas as pd
import re
from bs4 import BeautifulSoup
import time

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Main entry point for the agent."""
    logger.info("Starting {{ agent_name }}")
    
    try:
        # Fetch data from source
        source_type = "{{ source_type }}"
        source_value = "{{ source_value }}"
        
        logger.info(f"Scraping websites: {source_value}")
        
        # For multiple URLs, split them by comma
        urls = [url.strip() for url in source_value.split(',')]
        results = []
        
        for url in urls:
            # Respect rate limits
            if len(urls) > 1:
                time.sleep(12)  # Wait 12 seconds between requests
                
            logger.info(f"Scraping {url}")
            result = scrape_website(url)
            results.append(result)
            
        # Convert results to DataFrame
        df = pd.DataFrame(results)
        
        # Save output
        output_format = "{{ output_format }}"
        output_destination = "{{ output_destination }}"
        
        logger.info(f"Saving results to {output_destination}")
        save_data(df, output_format, output_destination)
        
        logger.info("Scraping completed successfully")
        return 0
        
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        return 1

def scrape_website(url):
    """Scrape data from a website."""
    result = {
        'url': url,
        'title': None,
        'description': None,
        'links': 0,
        'images': 0,
        'timestamp': datetime.now().isoformat(),
        'status': 'Unknown'
    }
    
    try:
        # Fetch the page
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # Parse HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extract title
        title = soup.find('title')
        if title:
            result['title'] = title.text.strip()
        
        # Extract description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            result['description'] = meta_desc.get('content', '')
        
        # Count links
        links = soup.find_all('a', href=True)
        result['links'] = len(links)
        
        # Count images
        images = soup.find_all('img')
        result['images'] = len(images)
        
        # Update status
        result['status'] = 'Success'
        
    except Exception as e:
        logger.error(f"Error scraping {url}: {str(e)}")
        result['status'] = f"Error: {str(e)}"
    
    return result

def save_data(data, output_format, output_destination):
    """Save processed data to the specified destination."""
    # Create output directory if it doesn't exist
    output_dir = os.path.dirname(output_destination)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    if output_format == "csv":
        data.to_csv(output_destination, index=False)
    elif output_format == "json":
        data.to_json(output_destination, orient='records', indent=2)
    else:
        # Default to CSV
        data.to_csv(output_destination, index=False)

if __name__ == "__main__":
    sys.exit(main())