#!/usr/bin/env python3
"""
web_analysis_agent

A simple agent generated by AutoGenesis.
"""

import os
import sys
import logging
import requests
from datetime import datetime
import pandas as pd
import re
from bs4 import BeautifulSoup
import time

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Main entry point for the agent."""
    logger.info("Starting web_analysis_agent")
    
    try:
        # Fetch data from source
        source_type = "url_list"
        source_value = "https://www.phxbistro.com/"
        
        logger.info(f"Analyzing website: {source_value}")
        
        # For multiple URLs, split them by comma
        urls = [url.strip() for url in source_value.split(',')]
        results = []
        
        for url in urls:
            # Respect rate limits
            if len(urls) > 1:
                time.sleep(12)  # Wait 12 seconds between requests
                
            logger.info(f"Fetching {url}")
            result = analyze_website(url)
            results.append(result)
            
        # Convert results to DataFrame
        df = pd.DataFrame(results)
        
        # Save output
        output_format = "csv"
        output_destination = "outdated_sites_analysis.csv"
        
        logger.info(f"Saving analysis to {output_destination}")
        save_data(df, output_format, output_destination)
        
        logger.info("Analysis completed successfully")
        return 0
        
    except Exception as e:
        logger.error(f"Error: {str(e)}")
        return 1

def analyze_website(url):
    """Analyze a website to determine when it was last updated."""
    result = {
        'url': url,
        'last_modified': None,
        'copyright_year': None,
        'latest_date_in_content': None,
        'is_outdated': False,
        'estimated_last_update': None,
        'age_in_years': None,
        'status': 'Unknown'
    }
    
    try:
        # Fetch the page
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # Check HTTP last-modified header
        if 'last-modified' in response.headers:
            last_modified = response.headers['last-modified']
            result['last_modified'] = last_modified
            last_modified_date = datetime.strptime(last_modified, '%a, %d %b %Y %H:%M:%S %Z')
            result['estimated_last_update'] = last_modified_date.strftime('%Y-%m-%d')
        
        # Parse HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Look for copyright year
        copyright_pattern = re.compile(r'[cC]opyright|\(c\)|©\s*(?:\doutdated_sites_analysis.csv-)?([12][0-9]csv)')
        copyright_text = soup.find(text=copyright_pattern)
        if copyright_text:
            match = copyright_pattern.search(copyright_text)
            if match:
                result['copyright_year'] = match.group(1)
        
        # Look for dates in the content
        date_pattern = re.compile(r'([0-3]?[0-9][/.][0-3]?[0-9][/.](?:20)?[12][0-9])|([12][0-9]{3}[-/.][0-3]?[0-9][-/.][0-3]?[0-9])')
        dates = []
        
        for text in soup.stripped_strings:
            matches = date_pattern.findall(text)
            for match in matches:
                date_str = match[0] if match[0] else match[1]
                dates.append(date_str)
        
        if dates:
            result['latest_date_in_content'] = max(dates)
        
        # Check meta tags
        meta_tags = soup.find_all('meta')
        for tag in meta_tags:
            if tag.get('name') and tag.get('name').lower() == 'date':
                result['meta_date'] = tag.get('content')
            if tag.get('name') and tag.get('name').lower() == 'dcterms.modified':
                result['dcterms_modified'] = tag.get('content')
        
        # Determine estimated last update
        current_year = datetime.now().year
        latest_year = None
        
        if result['copyright_year']:
            latest_year = int(result['copyright_year'])
        
        if result['last_modified']:
            last_modified_year = datetime.strptime(result['last_modified'], '%a, %d %b %Y %H:%M:%S %Z').year
            if not latest_year or last_modified_year > latest_year:
                latest_year = last_modified_year
        
        if latest_year:
            age_in_years = current_year - latest_year
            result['age_in_years'] = age_in_years
            result['is_outdated'] = age_in_years >= 5
            result['status'] = 'Outdated' if result['is_outdated'] else 'Current'
        
        if not result['estimated_last_update'] and latest_year:
            result['estimated_last_update'] = f"{latest_year}-01-01"
        
    except Exception as e:
        logger.error(f"Error analyzing {url}: {str(e)}")
        result['status'] = f"Error: {str(e)}"
    
    return result

def save_data(data, output_format, output_destination):
    """Save processed data to the specified destination."""
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_destination) or '.', exist_ok=True)
    
    if output_format == "csv":
        data.to_csv(output_destination, index=False)
    elif output_format == "json":
        data.to_json(output_destination, orient='records', indent=2)
    else:
        # Default to CSV
        data.to_csv(output_destination, index=False)

if __name__ == "__main__":
    sys.exit(main())
