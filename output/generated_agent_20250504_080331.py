#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
web_scraping_agent

A simple agent generated by AutoGenesis.
Improved version: handles errors better, structured consistently, scalable.
"""

import os
import sys
import logging
import requests
import json
from datetime import datetime
import pandas as pd
from bs4 import BeautifulSoup
import time

# Configurable delay between URL requests
DELAY_SECONDS = 12

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """Main entry point for the agent."""
    logger.info("Starting web_scraping_agent")

    try:
        # Fetch data from source
        source_type = "url_list"
        input_sources = "https://www.example.com,https://www.python.org,https://www.phxbistro.com"

        logger.info(f"Processing sources: {input_sources}")
        sources = [s.strip() for s in input_sources.split(',') if s.strip()]

        if not sources:
            logger.warning("No valid sources provided. Exiting.")
            return 1

        results = []

        for index, source in enumerate(sources):
            if index > 0:
                time.sleep(DELAY_SECONDS)  # Delay between requests

            logger.info(f"Processing: {source}")
            result = process_source(source, "url")
            results.append(result)

        df = pd.DataFrame(results)

        # Save output
        output_format = "csv"
        output_destination = "websites_info.csv"

        logger.info(f"Saving results to {output_destination}")
        save_data(df, output_format, output_destination)

        logger.info("Processing completed successfully.")
        return 0

    except Exception as e:
        logger.error(f"Unhandled error: {str(e)}")
        return 1


def process_source(source, source_type):
    """Process a source based on its type."""
    if source_type in {"url", "url_list"}:
        return process_url(source)
    elif source_type == "file":
        return process_file(source)
    else:
        raise ValueError(f"Unsupported source type: {source_type}")


def process_url(url):
    """Process a URL by fetching and extracting data."""
    result = {
        'url': url,
        'timestamp': datetime.now().isoformat(),
        'status': 'Unknown',
        'title': '',
        'description': ''
    }

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        title_tag = soup.find('title')
        result['title'] = title_tag.text.strip() if title_tag else ''

        meta_desc = soup.find('meta', attrs={'name': 'description'})
        result['description'] = meta_desc.get('content', '') if meta_desc else ''

        result['status'] = 'Success'

    except requests.exceptions.RequestException as e:
        logger.error(f"Network error processing {url}: {str(e)}")
        result['status'] = f"Network error: {str(e)}"

    except Exception as e:
        logger.error(f"Unexpected error processing {url}: {str(e)}")
        result['status'] = f"Error: {str(e)}"

    return result


def process_file(file_path):
    """Process a file by reading and extracting data."""
    result = {
        'file': file_path,
        'timestamp': datetime.now().isoformat(),
        'status': 'Unknown'
    }

    try:
        ext = os.path.splitext(file_path)[1].lower()

        if ext == '.csv':
            df = pd.read_csv(file_path)
            result['rows'] = len(df)
            result['columns'] = len(df.columns)
            result['status'] = 'Success'

        elif ext == '.json':
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                result['data_type'] = type(data).__name__
                result['status'] = 'Success'

        else:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                result['size'] = len(content)
                result['status'] = 'Success'

    except Exception as e:
        logger.error(f"Error processing file {file_path}: {str(e)}")
        result['status'] = f"Error: {str(e)}"

    return result


def save_data(data, output_format, output_destination):
    """Save processed data to the specified destination."""
    os.makedirs(os.path.dirname(output_destination) or '.', exist_ok=True)

    if output_format == "csv":
        data.to_csv(output_destination, index=False)
    elif output_format == "json":
        data.to_json(output_destination, orient='records', indent=2)
    else:
        logger.warning(f"Unsupported format '{output_format}', defaulting to CSV.")
        data.to_csv(output_destination, index=False)


if __name__ == "__main__":
    sys.exit(main())
